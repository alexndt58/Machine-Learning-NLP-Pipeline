{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04_eval.ipynb: Model Evaluation & Visualization\n",
    "\n",
    "# This notebook loads cross-validation results, computes evaluation metrics, and generates key figures for the report.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from joblib import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.preprocess import build_pipeline\n",
    "from src.models import svm\n",
    "\n",
    "if Path(\"../reports/results_table2.csv\").exists():\n",
    "    results = pd.read_csv(\"../reports/results_table2.csv\")\n",
    "elif Path(\"../results_table2.csv\").exists():\n",
    "    results = pd.read_csv(\"../results_table2.csv\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"results_table2.csv not found in expected locations.\")\n",
    "\n",
    "print(results.head())\n",
    "\n",
    "# results = pd.read_csv(\"../reports/results_table2.csv\")\n",
    "# print(results.head())\n",
    "\n",
    "\n",
    "# Adjust file path as needed\n",
    "#results = pd.read_csv(\"results_table2.csv\")  # Or the CSV generated by gather_results.py\n",
    "#print(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot confusion matrix for the best pipeline\n",
    "y_true = pd.read_csv(\"../data/test_labels.csv\")['label_name']  # Load your true labels (from held-out data/split)\n",
    "y_pred = pd.read_csv(\"../reports/best_model_predictions.csv\")['pred_label']   # Load predictions from the best model\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(set(y_true)))\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix (Best Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figures/cm_best.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true)))\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(set(y_true)))\n",
    "# disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "# plt.title(\"Confusion Matrix (Best Model)\")\n",
    "# plt.savefig(\"../reports/figures/cm_best.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot ROC curve (multi-class, one-vs-rest)\n",
    "# You may need to binarize y for multiclass\n",
    "# For each class:\n",
    "# fpr, tpr, _ = roc_curve(y_true_binary, y_score_for_this_class)\n",
    "# plt.plot(fpr, tpr, label=f'Class {i}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title(\"ROC Curve (Best Model)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../reports/figures/roc_best.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your cleaned data\n",
    "\n",
    "df = pd.read_feather(\"../data/tweets.feather\")\n",
    "X = df['text']\n",
    "y = df['label_name']\n",
    "\n",
    "# Use stratified split for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your best pipeline\n",
    "\n",
    "pipe = build_pipeline(\"../conf/rep_word_tfidf.yaml\")\n",
    "pipe.steps.append((\"clf\", svm()))\n",
    "pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test predictions\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeebdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved pipeline\n",
    "\n",
    "# Load your saved pipeline (adjust the path and filename)\n",
    "pipe = load(\"../reports/best_model.joblib\")\n",
    "X_test = pd.read_feather(\"../data/test.feather\")['text']\n",
    "y_true = pd.read_feather(\"../data/test.feather\")['label_name']\n",
    "y_pred = pipe.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test predictions\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"text\": X_test,\n",
    "    \"true_label\": y_test,\n",
    "    \"pred_label\": y_pred\n",
    "}).to_csv(\"../reports/best_model_predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plotting\n",
    "\n",
    "y_true = y_test\n",
    "# Or, if loading from CSV:\n",
    "# y_true = pd.read_csv(\"../reports/best_model_predictions.csv\")[\"true_label\"]\n",
    "# y_pred = pd.read_csv(\"../reports/best_model_predictions.csv\")[\"pred_label\"]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(set(y_true)))\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix (Best Model)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figures/cm_best.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

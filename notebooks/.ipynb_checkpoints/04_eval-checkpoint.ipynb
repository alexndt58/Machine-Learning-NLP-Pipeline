{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04_eval.ipynb: Model Evaluation & Visualization\n",
    "\n",
    "# This notebook loads cross-validation results, computes evaluation metrics, and generates key figures for the report.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Adjust file path as needed\n",
    "results = pd.read_csv(\"results_table2.csv\")  # Or the CSV generated by gather_results.py\n",
    "print(results.head())\n",
    "\n",
    "# Example: Plot confusion matrix for the best pipeline\n",
    "y_true = ...   # Load your true labels (from held-out data/split)\n",
    "y_pred = ...   # Load predictions from the best model\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(set(y_true)))\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix (Best Model)\")\n",
    "plt.savefig(\"../reports/figures/cm_best.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Example: Plot ROC curve (multi-class, one-vs-rest)\n",
    "# You may need to binarize y for multiclass\n",
    "# For each class:\n",
    "# fpr, tpr, _ = roc_curve(y_true_binary, y_score_for_this_class)\n",
    "# plt.plot(fpr, tpr, label=f'Class {i}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title(\"ROC Curve (Best Model)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../reports/figures/roc_best.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

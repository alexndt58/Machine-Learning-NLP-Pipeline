{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cada0-6428-4fe1-a6d5-0d9f99167bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CETM47 AS2: Exploratory Data Analysis (EDA)\n",
    "# Project: Real-Time Twitter Topic Classification  \n",
    "# Author: <Your Name & Student ID>  \n",
    "# Date: <today's date>\n",
    "\n",
    "## 1. Introduction\n",
    "#This notebook explores the structure, quality, and content of the raw tweet dataset for the NewsPulse Analytics project. The goal is to assess readiness for modeling, identify risks, and document cleaning/preprocessing choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38dc52a-7d8e-4b26-9b28-0493fa15056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Import Libraries and Set Up Paths\n",
    "#import all necessary Python libraries for data manipulation and plotting. We also ensure output folders exist for saving figures.\n",
    "import pandas as pd             # For data manipulation\n",
    "import numpy as np              # For numeric operations\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import seaborn as sns           # For advanced plots\n",
    "from pathlib import Path        # For cross-platform file paths\n",
    "import re                       # For regex\n",
    "import html                     # For HTML unescaping\n",
    "import os\n",
    "#from pathlib import Path\n",
    "\n",
    "\n",
    "# Set up figure output path and create it if it doesn't exist\n",
    "fig_dir = Path(\"../reports/figures\")\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd44a8-63f9-4cc3-a738-48d23ab49ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Load Raw Data\n",
    "#load the raw tweet data from JSON. The file is expected in `data/raw/`.\n",
    "from pathlib import Path\n",
    "data_path = Path(\"data/raw/CETM47_24_5-AS2-Data.json\")\n",
    "df = pd.read_json(data_path)\n",
    "print(f\"Loaded {len(df)} tweets.\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "# data_path = Path(\"data/raw/CETM47_24_5-AS2-Data.json\")  # Path to your raw data file\n",
    "# df = pd.read_json(data_path, lines=True)                # Load JSON lines as DataFrame\n",
    "# print(f\"Loaded {len(df)} tweets.\")\n",
    "# df.head()  # Display first few rows for a sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1641a-79fb-471d-994e-c38c761e41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Data Audit: Columns, Nulls, and Duplicates\n",
    "#This cell checks for null values, duplicate IDs and texts to identify quality issues.\n",
    "print(\"Columns:\", list(df.columns))       # Print all column names\n",
    "print(df.info())                          # Info: dtypes, non-null counts\n",
    "print(\"Nulls per column:\\n\", df.isnull().sum())              # Number of nulls in each column\n",
    "print(\"Duplicates by 'id':\", df['id'].duplicated().sum())    # Duplicate tweet IDs\n",
    "print(\"Duplicates by 'text':\", df['text'].duplicated().sum())# Duplicate tweet texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e09f9-5fe4-4f5f-9b3b-0ab3377ccbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Remove Duplicates and Short Tweets\n",
    "# Remove tweets with duplicate texts and tweets with fewer than 3 tokens (which are likely noise).\n",
    "df = df.drop_duplicates(subset='text')\n",
    "print(f\"Rows after dropping duplicate texts: {len(df)}\")\n",
    "\n",
    "# Add a column for token count per tweet\n",
    "df['token_len'] = df['text'].str.split().apply(len)\n",
    "short_tweets = df[df['token_len'] < 3]            # Identify short tweets\n",
    "print(f\"Tweets with fewer than 3 tokens: {len(short_tweets)}\")\n",
    "\n",
    "# Remove short tweets\n",
    "df = df[df['token_len'] >= 3].copy()\n",
    "print(f\"Rows after removing short tweets: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91f8b5-737c-4c5a-ab92-60a12323765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Class Distribution\n",
    "# Visualize the class imbalance in the dataset with a barplot.\n",
    "plt.figure(figsize=(8,4))\n",
    "# Use Seaborn's countplot to show the number of samples per class\n",
    "#sns.countplot(x='label_name', data=df, order=df['label_name'].value_counts().index, palette='Set2')\n",
    "sns.countplot(x='label_name', data=df, order=df['label_name'].value_counts().index, hue='label_name', palette='Set2', legend=False)\n",
    "\n",
    "#sns.countplot(x='label_name', data=df, order=df['label_name'].value_counts().index)\n",
    "plt.title(\"Tweet class distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / \"class_bar.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286cbde-eb9c-44d3-8e9b-441e6e81945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Tweet Length Statistics\n",
    "# Explore distribution of tweet length (in tokens and characters).\n",
    "df['char_len'] = df['text'].str.len()    # Add a column for character count\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,4))\n",
    "# Plot histogram of token counts\n",
    "sns.histplot(df['token_len'], bins=30, kde=True, ax=axs[0], color='steelblue')\n",
    "axs[0].set_title(\"Token count per tweet\")\n",
    "# Plot histogram of character counts\n",
    "sns.histplot(df['char_len'], bins=30, kde=True, ax=axs[1], color='orange')\n",
    "axs[1].set_title(\"Character count per tweet\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / \"length_hist.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print descriptive statistics\n",
    "print(\"Token stats:\")\n",
    "print(df['token_len'].describe(percentiles=[.5, .95, 1]))\n",
    "print(\"Char stats:\")\n",
    "print(df['char_len'].describe(percentiles=[.5, .95, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c141e-995c-4407-b156-355ff0324a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Tweet Volume Over Time\n",
    "# Check if there are spikes or gaps in tweet collection (e.g. due to events like COVID).\n",
    "df['date'] = pd.to_datetime(df['date'])    # Ensure dates are parsed as datetime objects\n",
    "timeline = df.set_index('date').resample('D').size()   # Count tweets per day\n",
    "plt.figure(figsize=(12,3))\n",
    "timeline.plot()\n",
    "plt.title(\"Tweets per day (time series)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir / \"timeline.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dc16c-5172-4968-bb2f-33373a12b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Top Hashtags by Class\n",
    "# Extract hashtags from tweets and show the most frequent per topic, to understand topic vocabulary.\n",
    "import collections\n",
    "\n",
    "def extract_hashtags(text):\n",
    "    \"\"\"Extract all hashtags from a tweet.\"\"\"\n",
    "    return re.findall(r\"#\\w+\", text)\n",
    "\n",
    "hashtag_counts = {}\n",
    "for topic in df['label_name'].unique():\n",
    "    hashtags = sum(df.loc[df['label_name']==topic, 'text'].apply(extract_hashtags), [])\n",
    "    hashtag_counts[topic] = collections.Counter(hashtags).most_common(5)\n",
    "\n",
    "print(\"Top hashtags per class:\")\n",
    "for k, v in hashtag_counts.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e9991-7602-45a4-a4ba-89d0b16f8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Emoji Presence Check\n",
    "# Estimate the percentage of tweets containing at least one emoji.\n",
    "import emoji\n",
    "\n",
    "def has_emoji(s):\n",
    "    \"\"\"Return True if any emoji present in text.\"\"\"\n",
    "    return any(char in emoji.EMOJI_DATA for char in s)\n",
    "\n",
    "df['has_emoji'] = df['text'].apply(has_emoji)\n",
    "percent_emoji = df['has_emoji'].mean() * 100\n",
    "print(f\"Percent of tweets with emoji: {percent_emoji:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5339d9-c6a8-4781-b031-a4c4a65a5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Save Cleaned Data for Downstream Tasks\n",
    "# Save the cleaned DataFrame to a binary Feather file for efficient loading in later scripts.\n",
    "out_path = Path(\"data/tweets.feather\")\n",
    "df.reset_index(drop=True).to_feather(out_path)\n",
    "print(f\"Saved cleaned DataFrame to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97116418-72ba-49ed-9204-d621d2e41169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP Venv)",
   "language": "python",
   "name": "nlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
